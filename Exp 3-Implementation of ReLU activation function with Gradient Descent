#Implementation of ReLU activation function with Gradient Descent
import numpy as np
import matplotlib.pyplot as plt

# Define ReLU function
def ReLU_function(x):
    return np.maximum(0, x)

# Gradient Descent with ReLU
def gradient_descent_ReLU(x, y, lr, epochs):
    weights = 0  # Initialize weights
    for _ in range(epochs):
        y_pred = ReLU_function(x * weights)  # Predict using ReLU function
        error = y - y_pred  # Calculate the error
        gradient = np.where(x * weights > 0, 1, 0)  # ReLU derivative
        weights += lr * np.sum(error * x * gradient)  # Update weights
    return weights

# Data
x = np.array([2, 1, 0, 1, 2])
y = np.array([0, 0, 1, 1, 1])
lr = 0.1
epochs = 10

# Train the model
final_weights = gradient_descent_ReLU(x, y, lr, epochs)

# Generate predictions using ReLU
y_pred = ReLU_function(x * final_weights)



print("Final weights:", final_weights)
