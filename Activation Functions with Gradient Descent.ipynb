{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNz4E3x/weRzInvM0V1I92+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mkFcQq0YtX1O","executionInfo":{"status":"ok","timestamp":1738638779339,"user_tz":-330,"elapsed":513,"user":{"displayName":"Aditya Suryakar","userId":"04862705019986316821"}},"outputId":"33d6ae17-dec7-45df-eeaf-cd5acaa27c8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final weights: 0.0\n"]}],"source":["#implementation of step activation function with gradient Descent\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def step_function(x):\n","    return np.where(x >= 0, 1, 0)\n","\n","def gradient_descent_step(x, y, lr, epochs):\n","    weights = 0  # Initialize weights\n","    for _ in range(epochs):\n","        y_pred = step_function(x * weights)  # Predict using step function\n","        error = y - y_pred  # Calculate the error\n","        weights += lr * np.sum(error * x)  # Update weights\n","    return weights\n","\n","# Data\n","x = np.array([-2, 1, 0, 1, 2])\n","y = np.array([0, 0, 1, 1, 1])\n","lr = 0.1\n","epochs = 10\n","\n","# Train the model\n","final_weights = gradient_descent_step(x, y, lr, epochs)\n","\n","# Generate predictions using the step function\n","y_pred = step_function(x * final_weights)\n","\n","\n","\n","print(\"Final weights:\", final_weights)\n"]},{"cell_type":"code","source":["#implementation of sigmoid activation function with Gradient Descent\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def sigmoid_function(x):\n","    return 1/(1+np.exp(-x))\n","\n","def gradient_descent_sigmoid(x, y, lr, epochs):\n","    weights = 0  # Initialize weights\n","    for _ in range(epochs):\n","        y_pred = sigmoid_function(x * weights)  # Predict using sigmoid function\n","        error = y - y_pred  # Calculate the error\n","        weights += lr * np.sum(error * x)  # Update weights\n","    return weights\n","\n","# Data\n","x = np.array([-2, 1, 0, 1, 2])\n","y = np.array([0, 0, 1, 1, 1])\n","lr = 0.1\n","epochs = 10\n","\n","# Train the model\n","final_weights = gradient_descent_sigmoid(x, y, lr, epochs)\n","\n","# Generate predictions using the sigmoid function\n","y_pred = sigmoid_function(x * final_weights)\n","\n","\n","\n","print(\"Final weights:\", final_weights)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIvtd94YxAhC","executionInfo":{"status":"ok","timestamp":1737999158993,"user_tz":-330,"elapsed":5,"user":{"displayName":"Aditya Suryakar","userId":"04862705019986316821"}},"outputId":"467239b4-30f2-469b-e845-c8de774efcf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final weights: 0.8352046889815125\n"]}]},{"cell_type":"code","source":["#Implementation of ReLU activation function with Gradient Descent\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define ReLU function\n","def ReLU_function(x):\n","    return np.maximum(0, x)\n","\n","# Gradient Descent with ReLU\n","def gradient_descent_ReLU(x, y, lr, epochs):\n","    weights = 0  # Initialize weights\n","    for _ in range(epochs):\n","        y_pred = ReLU_function(x * weights)  # Predict using ReLU function\n","        error = y - y_pred  # Calculate the error\n","        gradient = np.where(x * weights > 0, 1, 0)  # ReLU derivative\n","        weights += lr * np.sum(error * x * gradient)  # Update weights\n","    return weights\n","\n","# Data\n","x = np.array([2, 1, 0, 1, 2])\n","y = np.array([0, 0, 1, 1, 1])\n","lr = 0.1\n","epochs = 10\n","\n","# Train the model\n","final_weights = gradient_descent_ReLU(x, y, lr, epochs)\n","\n","# Generate predictions using ReLU\n","y_pred = ReLU_function(x * final_weights)\n","\n","\n","\n","print(\"Final weights:\", final_weights)\n"],"metadata":{"id":"xXnSYtz8xw3l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737999144369,"user_tz":-330,"elapsed":1544,"user":{"displayName":"Aditya Suryakar","userId":"04862705019986316821"}},"outputId":"5a52326d-fa85-4ab5-c5bd-fd67f88f5658"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final weights: 0.0\n"]}]},{"cell_type":"code","source":["#Implementation of tanh activation function with Gradient Descent\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define tanh function\n","def tanh_function(x):\n","    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n","\n","\n","# Gradient Descent with tanh\n","def gradient_descent_tanh(x, y, lr, epochs):\n","    weights = 0  # Initialize weights\n","    for _ in range(epochs):\n","        y_pred = tanh_function(x * weights)  # Predict using tanh function\n","        error = y - y_pred  # Calculate the error\n","        gradient = np.where(x * weights > 0, 1, 0)  # ReLU derivative\n","        weights += lr * np.sum(error * x * gradient)  # Update weights\n","    return weights\n","\n","# Data\n","x = np.array([2, 1, 0, 1, 2])\n","y = np.array([0, 0, 1, 1, 1])\n","lr = 0.1\n","epochs = 10\n","\n","# Train the model\n","final_weights = gradient_descent_tanh(x, y, lr, epochs)\n","\n","# Generate predictions using tanh\n","y_pred = tanh_function(x * final_weights)\n","\n","\n","\n","print(\"Final weights:\", final_weights)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSIn_k5uhV_N","executionInfo":{"status":"ok","timestamp":1738683524796,"user_tz":480,"elapsed":397,"user":{"displayName":"Aditya Suryakar","userId":"04862705019986316821"}},"outputId":"d5a887ed-835e-4698-9a82-2e27cacd70b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final weights: 0.0\n"]}]},{"cell_type":"code","source":["#Student Performance Prediction using Activation Function with Gradient Descent\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#define sigmoid function\n","def sigmoid_function(x):\n","    return 1/(1+np.exp(-x))\n","\n","#define Relu function\n","def ReLU_function(study_hours):\n","    return np.maximum(0, x)\n","\n","#define Tanh function\n","def tanh_function(study_hours):\n","    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n","\n","#inputs\n","study_hours = np.linspace(1,10,100)\n","print(\"Study hours:\",study_hours)\n","#input the values of weight\n","w=np.random.rand()\n","#input the values of bias\n","b=np.random.rand()\n","lr=0.0001\n","epochs=1700\n","\n","#outputs\n","exam_scores = 5*study_hours + np.random.normal(0,2,100)\n","print(\"Exam scores:\",exam_scores)\n","\n","# Gradient Descent with tanh\n","def gradient_descent_(study_hours, exam_scores, lr, epochs):\n","    weights = 0  # Initialize weights\n","    for _ in range(epochs):\n","        y_pred = tanh_function(x * weights)  # Predict using tanh function\n","        error = y - y_pred  # Calculate the error\n","        gradient = np.where(x * weights > 0, 1, 0)  # ReLU derivative\n","        weights += lr * np.sum(error * x * gradient)  # Update weights\n","    return weights"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDD0Nnp3meOm","executionInfo":{"status":"ok","timestamp":1738684517706,"user_tz":480,"elapsed":1311,"user":{"displayName":"Aditya Suryakar","userId":"04862705019986316821"}},"outputId":"819e9283-045a-4948-ca3b-cb85851c74fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Study hours: [ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n","  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n","  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n","  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n","  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n","  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n","  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n","  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n","  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n","  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n","  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n","  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n","  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n","  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n","  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n","  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n","  9.72727273  9.81818182  9.90909091 10.        ]\n","Exam scores: [ 5.71557472  6.57611451  8.0751934   8.47124047  4.06284308  5.39707719\n","  8.75734326  9.20939008  9.66645901 16.79637207 10.68723557 12.27113128\n"," 12.36254898 12.21187341 10.73309787 13.33612026 10.72707684 12.25363551\n"," 12.21109109 13.80011192 18.72022622 10.81092416 16.37252038 12.22911371\n"," 14.96522718 18.54153756 16.94674186 15.11723772 16.29666531 19.54101368\n"," 17.17563037 19.52382627 19.63659823 18.6967993  24.74243363 22.17692895\n"," 17.31335119 22.19109045 20.94915434 24.4321394  21.5967767  23.40689075\n"," 25.10088365 26.27696493 22.59940719 24.78554298 24.95920029 25.0569779\n"," 30.3490903  28.08269069 25.20550482 30.01754208 32.88067603 31.15583961\n"," 26.50671461 29.03153185 32.98836775 29.49375198 32.25127522 33.36744993\n"," 30.41886633 32.60822202 26.6992835  31.58758835 33.58577279 32.04988818\n"," 38.26482261 32.5942627  35.02900194 36.62511752 39.7007284  34.40100297\n"," 40.05360023 38.2022843  36.67334633 40.01511604 39.94357394 38.79956625\n"," 40.59414962 40.13846372 41.59067105 43.14244317 45.44476091 40.25164173\n"," 47.44788493 39.73218804 43.7873389  45.72208896 45.56198374 44.20914641\n"," 45.49284641 45.37763449 45.6394523  48.97193147 48.4413037  46.79599899\n"," 50.43556339 49.70550813 51.17117878 51.25925768]\n"]}]}]}